00:00 Ana: Hola a todos. Bienvenidos una vez más al podcast. Hoy vamos a tratar un tema que seguro que a más de uno le ha quitado el sueño, literalmente, a las tres de la mañana. Vamos a hablar de por qué el logging, tal y como lo conocemos, es un auténtico desastre.
00:16 Carlos: Hola, Ana. Pues sí, es un tema frustrante. Boris Tane publicó hace poco un artículo titulado "Logging Sucks" y no se anda con chiquitas. Dice directamente que nuestros logs nos están mintiendo. No por maldad, sino porque simplemente no están preparados para la complejidad de hoy en día.
00:33 Ana: Es que es verdad. ¿Cuántas veces nos hemos visto haciendo un grep infinito buscando por qué falló un pago o por qué hubo un pico de latencia, para acabar encontrando solo mensajes vagos y marcas de tiempo que no dicen nada? Es como intentar ser detective con una mano atada a la espalda.
00:51 Carlos: Totalmente. Boris explica que el problema de raíz es que los logs se diseñaron para la era de los monolitos y servidores únicos, donde podías reproducir todo en local. Pero hoy, una sola petición de un usuario puede tocar 15 servicios, tres bases de datos y un par de cachés. Los logs de 2005 ya no sirven para el mundo de 2024.
01:12 Ana: De hecho, en el artículo pone un ejemplo de un simulador de caos. Para una sola petición con éxito se generan 13 líneas de log. Si tienes 10.000 usuarios concurrentes, eso son 130.000 líneas por segundo. Una locura de datos que en realidad no dicen nada útil porque les falta lo más importante: el contexto.
01:35 Carlos: Exacto, ese es el punto clave. Boris menciona que buscar por cadenas de texto está roto. Si buscas user123, puede que aparezca de mil formas distintas: con guion bajo, como un objeto JSON, entre corchetes... o peor aún, que un servicio de flujo abajo solo registre el ID del pedido y ya pierdas el rastro del usuario.
01:55 Ana: Es que al final, como dice él, los logs están optimizados para escribirse, no para consultarse. Al desarrollador le resulta fácil poner un console.log("el pago ha fallado"), pero no piensa en el pobre compañero que tendrá que descifrar eso durante una caída del sistema de madrugada.
02:11 Carlos: Por eso es importante definir bien de qué hablamos. Boris aclara términos que a veces confundimos. Por ejemplo, el structured logging o logueo estructurado, que es emitir logs como pares clave-valor, normalmente en JSON. Está bien, es necesario, pero dice que no es suficiente por sí solo.
02:27 Ana: También habla de la cardinalidad y la dimensionalidad, ¿verdad? Me pareció muy interesante. La cardinalidad es el número de valores únicos, como un ID de usuario que tiene millones, y la dimensionalidad es el número de campos. Para Boris, si quieres respuestas de verdad, necesitas alta dimensionalidad: muchos campos que den contexto.
02:49 Carlos: Ahí es donde entra su gran solución: los wide events o eventos anchos, que Stripe también llama "líneas de log canónicas". La idea es dejar de pensar en los logs como un diario de lo que hace el código y empezar a verlos como un registro estructurado de eventos de negocio.
03:06 Ana: Me encanta ese cambio de mentalidad. En lugar de 13 líneas dispersas, emites una sola línea gigante por cada servicio y petición. Una línea que contenga todo: el ID del usuario, su tipo de suscripción, el tiempo que tardó la base de datos, los feature flags que tenía activos... todo en un solo bloque de información.
03:26 Carlos: Imagínate la diferencia. Si un usuario premium se queja, haces una sola consulta y ves al instante que era su tercer intento de pago, que falló por fondos insuficientes y que además estaba usando el nuevo flujo de checkout. No hay que buscar más, la respuesta está ahí.
03:42 Ana: Pero claro, Carlos, aquí surge la duda que todos tenemos: el coste. Si metemos 50 campos por cada petición y tenemos miles de peticiones, la factura de la plataforma de observabilidad nos va a arruinar, ¿no?
03:55 Carlos: Es una preocupación muy real, y Boris la aborda con el concepto de tail sampling o muestreo selectivo. En lugar de guardar el 100% de los logs, que es carísimo, decides qué guardar al final de la petición.
04:08 Ana: Claro, las reglas son lógicas. Guarda siempre los errores, guarda siempre las peticiones lentas que superen un umbral de latencia y guarda siempre lo que venga de clientes VIP o cuentas de prueba internas. Para el resto, las peticiones que han ido rápido y bien, solo guardas un 1% o un 5% para tener estadísticas.
04:30 Carlos: Exacto. Así mantienes los costes bajo control, pero nunca pierdes el evento que realmente importa para debuguear un problema. Es el equilibrio perfecto.
04:39 Ana: Hay un punto en el artículo que me hizo gracia, y es cuando habla de OpenTelemetry. Mucha gente piensa que por usar OTel ya tiene la observabilidad solucionada, y Boris dice que ni de broma.
04:47 Carlos: Sí, dice que OpenTelemetry es solo el protocolo, el camión que transporta los datos. Pero el camión no decide qué cargar. Si tú no instrumentas tu código para añadir el contexto de negocio, OTel solo enviará telemetría vacía de forma muy estándar, pero inútil.
05:03 Ana: Al final el resumen es que debemos dejar de hacer arqueología entre archivos de texto y empezar a hacer analítica de datos sobre nuestro tráfico de producción. Boris incluso propone un patrón de implementación usando un middleware que va construyendo ese "evento ancho" durante todo el ciclo de vida de la petición.
05:19 Carlos: Es una forma mucho más madura de trabajar. Pasar de "creo que el usuario tuvo un problema" a ejecutar una consulta que te diga exactamente qué falló, a quién y por qué, en menos de un segundo.
05:32 Ana: Pues nos quedamos con esa idea: que tus logs dejen de mentirte y empiecen a decirte la verdad completa. Muchas gracias, Carlos, por desgranar este artículo conmigo.
05:43 Carlos: Un placer, Ana. Hasta la próxima.
05:46 Ana: Hasta la próxima a todos. ¡Gracias!